{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Let's use TensorFlow to Solve a common Classification problem with the MNIST DataSet a.k.a \"The Hello World! of Machine Learning\".**","metadata":{}},{"cell_type":"markdown","source":"> The MNIST DataSet is an already Pre-Processed DataSet that contains handwritten digits (from 1 to 10) as 28x28 Pixels Matrices, where each pixel represent the density of the color Black, from 0 - Completely Dark to 255 - Completely White.\n> We'll teach the Machine, through Supervised Learning, how to correctly classify the handwritten digits, or in other terms, to understand which number it's in front of, based on the color (Black) density of each of the 784 Pixels.","metadata":{}},{"cell_type":"markdown","source":"* **Importing the Required Libraries.**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\nimport tensorflow_datasets as tfds","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-22T17:53:47.876749Z","iopub.execute_input":"2023-02-22T17:53:47.877942Z","iopub.status.idle":"2023-02-22T17:53:47.883098Z","shell.execute_reply.started":"2023-02-22T17:53:47.877889Z","shell.execute_reply":"2023-02-22T17:53:47.881969Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"* **Importing the DataSet**","metadata":{}},{"cell_type":"code","source":"MnistData, MnistInfo = tfds.load(name = \"mnist\", with_info = True, as_supervised = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:47.885037Z","iopub.execute_input":"2023-02-22T17:53:47.885585Z","iopub.status.idle":"2023-02-22T17:53:47.975304Z","shell.execute_reply.started":"2023-02-22T17:53:47.885556Z","shell.execute_reply":"2023-02-22T17:53:47.974377Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"MnistTrain, MnistTest = MnistData[\"train\"], MnistData[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:47.977148Z","iopub.execute_input":"2023-02-22T17:53:47.977835Z","iopub.status.idle":"2023-02-22T17:53:47.983295Z","shell.execute_reply.started":"2023-02-22T17:53:47.977793Z","shell.execute_reply":"2023-02-22T17:53:47.982068Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Calculating the Validation Size\nValidationSamples = tf.cast(0.1*MnistInfo.splits[\"train\"].num_examples, tf.int64)\n\n#Storing the Test Size into a Variable\nTestSamples = tf.cast(MnistInfo.splits[\"test\"].num_examples, tf.int64)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:47.985292Z","iopub.execute_input":"2023-02-22T17:53:47.985621Z","iopub.status.idle":"2023-02-22T17:53:47.995020Z","shell.execute_reply.started":"2023-02-22T17:53:47.985593Z","shell.execute_reply":"2023-02-22T17:53:47.993878Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"* **Let's now Define a Function to Scale the Data in a Numerical Stable way (from 0 to 1).**","metadata":{}},{"cell_type":"code","source":"def ScaleData(Image, Label):\n    #Casting the Image into Float Type\n    Image = tf.cast(Image, tf.float32)\n    \n    #Scaling the Image and keeping it as a Float (the . after the Division keeps it float)\n    Image /= 255.\n    \n    return Image, Label","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:47.996263Z","iopub.execute_input":"2023-02-22T17:53:47.996590Z","iopub.status.idle":"2023-02-22T17:53:48.005138Z","shell.execute_reply.started":"2023-02-22T17:53:47.996562Z","shell.execute_reply":"2023-02-22T17:53:48.004200Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"* **Scaling Training (and Validation) Data as well as the Test Data**","metadata":{}},{"cell_type":"code","source":"ScaledTrain = MnistTrain.map(ScaleData)\nScaledTest = MnistTest.map(ScaleData)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:48.006325Z","iopub.execute_input":"2023-02-22T17:53:48.006601Z","iopub.status.idle":"2023-02-22T17:53:48.066949Z","shell.execute_reply.started":"2023-02-22T17:53:48.006575Z","shell.execute_reply":"2023-02-22T17:53:48.066061Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"* **Let's now Shuffle the Data and Split the Train DataSet into Train and Validation Data**","metadata":{}},{"cell_type":"code","source":"#Initializing the Buffer, which is going to be used to set the Sample Size that is going to be Shuffled Each Time, since with enormous DataSet we won't be able to Shuffle them all at once.\n#NOTE: \n#ShuffleSize = BufferSize = Buffer\n#ShuffleSize = 1 -> No Shuffling actually happening.\n#ShuffleSize >= TotalSampleSize -> Shuffling will take place at Once and Uiformly.\n#1 < ShuffleSize < TotalSampleSize -> Shuffling will in Different Batches and will Optimize the Computational Power of the Machine.\n\nBuffer = 10000\n\nShuffledTrain = ScaledTrain.shuffle(Buffer)\n\n#Kinda Pointless to Shuffle the Test Set, but Whatever...\nShuffledTest = ScaledTest.shuffle(Buffer)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:48.068088Z","iopub.execute_input":"2023-02-22T17:53:48.068388Z","iopub.status.idle":"2023-02-22T17:53:48.075358Z","shell.execute_reply.started":"2023-02-22T17:53:48.068361Z","shell.execute_reply":"2023-02-22T17:53:48.074369Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#Extracting the Validation DataSet and Re-Creating the Train Data without the Validation Data Points.\n\nShuffledValidation = ShuffledTrain.take(ValidationSamples)\nShuffledTrainOnly = ShuffledTrain.skip(ValidationSamples)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:48.077049Z","iopub.execute_input":"2023-02-22T17:53:48.077505Z","iopub.status.idle":"2023-02-22T17:53:48.090750Z","shell.execute_reply.started":"2023-02-22T17:53:48.077466Z","shell.execute_reply":"2023-02-22T17:53:48.089750Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"* **Batching the DataSet for the Mini-Batch Stochasting Gradient Descent**","metadata":{}},{"cell_type":"code","source":"#NOTE:\n#BatchSize = 1 -> Stochastic Gradient Descent\n#BatchSize = TotalSampleSize -> (SingleBatch) Gradient Descent\n# 1 < BatchSize < TotalSampleSize -> MiniBatch Gradient Descent\n\nBatchSize = 100\n\n#Batching\nBatchedTrain = ShuffledTrainOnly.batch(BatchSize)\n\n#Since we'll only Forward-Propagate on the Validation and Test Sets, we want them to not be Batched or BatchSize = TotalSamples.\n#So, Since the Model will want the Validation Set in also Batch Format, we need to Batch it with its TotalSampleSize as BatchSize.\nBatchedValidation = ShuffledValidation.batch(ValidationSamples)\n\n#The Same Applies for the Test Data:\nBatchedTest = ShuffledTest.batch(TestSamples)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:48.095157Z","iopub.execute_input":"2023-02-22T17:53:48.095632Z","iopub.status.idle":"2023-02-22T17:53:48.175189Z","shell.execute_reply.started":"2023-02-22T17:53:48.095602Z","shell.execute_reply":"2023-02-22T17:53:48.174382Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#The Validation Data must have the same Shape and Properties as the Train and Test Data.\n#The Mnist Data is an Iterable and in 2-Tuples Format since we set as_supervised = True.\n#Therefore we must Extract and Convert the Inputs and Targets Accordingly.\n\nValidationInputs, ValidationTargets = next(iter(BatchedValidation))\n\n#iter is used to make the Validation Data an Iterator, and next is used to load the next batch into the Iterable.\n#Since there's only one Batch, it'll Load the Inputs and Targets.","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:48.176472Z","iopub.execute_input":"2023-02-22T17:53:48.177283Z","iopub.status.idle":"2023-02-22T17:53:49.469337Z","shell.execute_reply.started":"2023-02-22T17:53:48.177256Z","shell.execute_reply":"2023-02-22T17:53:49.468370Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"2023-02-22 17:53:49.073570: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* **Outlining the Model.**","metadata":{}},{"cell_type":"code","source":"InputSize = 784 #One Input Neuron for each Pixel.\nOutputSize = 10 #Ten, since they are our Number of Different Ouputs (Numbers from 1 to 10).\nHiddenLayersSize = 50 #Size of the Hidden Layers, we will have 2 and assume they'll have same Size.\n\n#Defining the Model.\nModel = tf.keras.Sequential([\n                            #This will Flatten into a Vector our Input Tensor of Size (28, 28, 1).\n                            tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n                            #Dense Takes the Inputs and Calculates the Dot Product of the Inputs and Weights and adds the Bias.\n                            #This is where we can apply the Activation Function.\n                            #The Process is done Twice since we will have 2 Hidden Layers of the same Size with the same Activation Function.\n                            tf.keras.layers.Dense(HiddenLayersSize, activation = \"relu\"),\n                            tf.keras.layers.Dense(HiddenLayersSize, activation = \"relu\"),\n                            #Defining the Output Layer that will use SoftMax Activation Function and will have size 10.\n                            #SoftMax is used to turn Values into Probabilities.\n                            tf.keras.layers.Dense(OutputSize, activation = \"softmax\"),\n                            ])\n\n#Setting the Optimizer and the Loss Function.\n#We'll use the SparseCategoricalCrossEntropy Loss Function, since it applies One Hot Encoding to our Outputs.\nModel.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:49.473878Z","iopub.execute_input":"2023-02-22T17:53:49.474196Z","iopub.status.idle":"2023-02-22T17:53:49.523115Z","shell.execute_reply.started":"2023-02-22T17:53:49.474168Z","shell.execute_reply":"2023-02-22T17:53:49.522234Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"* **Training the Model.**","metadata":{}},{"cell_type":"code","source":"#Setting the Number of Epochs.\nEpochs = 5\n\n#Training the Model.\nModel.fit(BatchedTrain, epochs = Epochs, validation_data = (ValidationInputs, ValidationTargets), verbose = 2)\n\n#1) At the Beginning of Each Epoch, the Training Loss will be set to 0.\n#2) The Algorithm will iterate over the preset number of Batches from TrainData.\n#3) The Weights and Biases will be Updated as many times as there are Batches.\n#4) We will get a Value for the Loss Function, Indicating how the Training is going.\n#5) We will also see the Training Accuracy thanks to the Verbose.\n#6) At the End of each Epoch the Algorithm will Forward Propagate the whole Validation Set.\n#7) When we'll reach the Maximum number of Epochs the Training will be Over.","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:53:49.524212Z","iopub.execute_input":"2023-02-22T17:53:49.524563Z","iopub.status.idle":"2023-02-22T17:54:09.320111Z","shell.execute_reply.started":"2023-02-22T17:53:49.524533Z","shell.execute_reply":"2023-02-22T17:54:09.319185Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch 1/5\n540/540 - 5s - loss: 0.4245 - accuracy: 0.8800 - val_loss: 0.2337 - val_accuracy: 0.9327 - 5s/epoch - 9ms/step\nEpoch 2/5\n540/540 - 3s - loss: 0.1949 - accuracy: 0.9438 - val_loss: 0.1627 - val_accuracy: 0.9533 - 3s/epoch - 6ms/step\nEpoch 3/5\n540/540 - 3s - loss: 0.1467 - accuracy: 0.9577 - val_loss: 0.1348 - val_accuracy: 0.9622 - 3s/epoch - 6ms/step\nEpoch 4/5\n540/540 - 3s - loss: 0.1206 - accuracy: 0.9651 - val_loss: 0.1107 - val_accuracy: 0.9690 - 3s/epoch - 6ms/step\nEpoch 5/5\n540/540 - 3s - loss: 0.0982 - accuracy: 0.9710 - val_loss: 0.1059 - val_accuracy: 0.9678 - 3s/epoch - 6ms/step\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fce04700490>"},"metadata":{}}]},{"cell_type":"markdown","source":"You can pay Around the the Hidden Layers' Sizes and look at the Training Results to see if you can Maximize Accuracy.\n\nYou can also, by doing this, trying to force OverFitting and try to spot it by looking at the Training Results (When both Training and Validation Losses are Decreasing, but all of the Sudden the Validation Loss starts Increasing).\n\nYou can, in Short:\n- Play around with the Width of the Algorithm (Layer Size) in any way you want.\n- Play around with the Depth of the Algorithm (Number of Layers) in any way you want.\n- Try out different Activation Functions for each Hidden Layer.\n- Tweak the Batch Size from 1 to 10000.\n- Adjust the Learning Rate from as low as 0.0001.","metadata":{}},{"cell_type":"markdown","source":"* **Testing the Model.**","metadata":{}},{"cell_type":"code","source":"#We'll Test our Model with the Test Data with the Evaluate Method, which will return the Loss and Metrics for the Model in \"Test Mode\".\nTestLoss, TestAccuracy = Model.evaluate(BatchedTest)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:54:09.321563Z","iopub.execute_input":"2023-02-22T17:54:09.322400Z","iopub.status.idle":"2023-02-22T17:54:10.056070Z","shell.execute_reply.started":"2023-02-22T17:54:09.322354Z","shell.execute_reply":"2023-02-22T17:54:10.054961Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 722ms/step - loss: 0.1205 - accuracy: 0.9646\n","output_type":"stream"}]},{"cell_type":"code","source":"#Accuracy and Loss we are Expecting for Real World Scenarios.\nprint(f\"Test Loss: {TestLoss} \\nTestAccuracy: {TestAccuracy*100}%\")","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:54:10.058276Z","iopub.execute_input":"2023-02-22T17:54:10.058644Z","iopub.status.idle":"2023-02-22T17:54:10.063872Z","shell.execute_reply.started":"2023-02-22T17:54:10.058613Z","shell.execute_reply":"2023-02-22T17:54:10.062589Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Test Loss: 0.12049306929111481 \nTestAccuracy: 96.46000266075134%\n","output_type":"stream"}]}]}